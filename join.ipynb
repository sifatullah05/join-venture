{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\nimport os\nimport json\nimport numpy as np\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nfrom dotenv import load_dotenv\nfrom langdetect import detect\nfrom PyPDF2 import PdfReader\nimport faiss\nimport torch\nfrom transformers import BertTokenizer, BertForQuestionAnswering\nfrom transformers import pipeline\n\n# Load environment variables (e.g., API keys, model paths)\nload_dotenv()\n\n# Define the path to the financial policy PDF document\npdf_path = \"জ্বালানি নীতিমালা ২০২৫ (গেজেট)\"\n\n# Load BERT model and tokenizer for Question Answering\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased')\n\n# Embeddings Backend Setup (SentenceTransformers for embeddings)\nfrom sentence_transformers import SentenceTransformer\n\n# Define embedding model backend (can be 'sentence_transformers' or OpenAI embeddings)\nEMBEDDING_BACKEND = os.getenv(\"EMBEDDING_BACKEND\", \"sentence_transformers\")\nEMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"all-MiniLM-L6-v2\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n\n# Define Faiss vector database for storing and searching text chunks\nclass EmbeddingEngine:\n    def __init__(self):\n        self.backend = EMBEDDING_BACKEND\n        if self.backend == \"sentence_transformers\":\n            self.model = SentenceTransformer(EMBEDDING_MODEL)\n        else:\n            if OPENAI_API_KEY:\n                import openai\n                openai.api_key = OPENAI_API_KEY\n            else:\n                self.model = None\n\n    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n        \"\"\" Embed the given texts into numerical vectors \"\"\"\n        if not texts:\n            return []\n        if self.backend == \"sentence_transformers\":\n            embeddings = self.model.encode(texts, convert_to_numpy=True)\n            return embeddings.astype(\"float32\").tolist()\n        else:\n            embeddings = []\n            for text in texts:\n                response = openai.Embedding.create(model=\"text-embedding-ada-002\", input=text)\n                embeddings.append(response['data'][0]['embedding'])\n            return embeddings\n\n# Utility functions\n\ndef detect_language(text: str) -> str:\n    \"\"\" Detect the language of the given text (e.g., English, Bengali) \"\"\"\n    try:\n        return detect(text)\n    except Exception:\n        return \"en\"\n\ndef chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n    \"\"\" Chunk the text into smaller parts for processing \"\"\"\n    chunks = []\n    start = 0\n    while start < len(text):\n        end = min(start + chunk_size, len(text))\n        chunk = text[start:end].strip()\n        if chunk:\n            chunks.append(chunk)\n        start = end - overlap\n    return chunks\n\n# ------------------------\n# Ingest PDF & Build Index\n# ------------------------\n\ndef read_pdf_by_page(pdf_path: str) -> List[Dict[str, Any]]:\n    \"\"\" Extract text from each page of the PDF \"\"\"\n    reader = PdfReader(pdf_path)\n    pages = []\n    for i, page in enumerate(reader.pages):\n        try:\n            text = page.extract_text() or \"\"\n        except Exception:\n            text = \"\"\n        pages.append({\"page_number\": i + 1, \"text\": text})\n    return pages\n\ndef build_faiss_index(pages: List[Dict[str, Any]], index_dir: str, chunk_size=1000, overlap=200):\n    \"\"\" Create a Faiss index from the embedded text chunks \"\"\"\n    index_dir = Path(index_dir)\n    index_dir.mkdir(parents=True, exist_ok=True)\n    docs, metadatas = [], []\n    \n    for p in pages:\n        page_num = p[\"page_number\"]\n        text = p[\"text\"]\n        chunks = chunk_text(text, chunk_size, overlap)\n        for i, c in enumerate(chunks):\n            docs.append(c)\n            metadatas.append({\"page\": page_num, \"chunk\": i, \"preview\": c[:300]})\n\n    if len(docs) == 0:\n        raise ValueError(\"No text extracted from PDF.\")\n    \n    # Create embeddings\n    engine = EmbeddingEngine()\n    embeds = engine.embed_texts(docs)\n    xb = np.array(embeds).astype(\"float32\")\n    dim = xb.shape[1]\n    \n    # Create a Faiss index and save it\n    index = faiss.IndexFlatL2(dim)\n    index.add(xb)\n    faiss.write_index(index, str(index_dir / \"index.faiss\"))\n    \n    with open(index_dir / \"metadata.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump({\"docs\": docs, \"metadatas\": metadatas}, f, ensure_ascii=False, indent=2)\n    \n    print(f\"Index and metadata saved to {index_dir}\")\n\n# ------------------------\n# Question Answering with BERT\n# ------------------------\n\ndef qa_bert_model(query: str, context: str):\n    \"\"\" Use BERT model for question answering given a context \"\"\"\n    inputs = tokenizer(query, context, return_tensors=\"pt\", truncation=True, padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        answer_start_scores = outputs.start_logits\n        answer_end_scores = outputs.end_logits\n    \n    start = torch.argmax(answer_start_scores)\n    end = torch.argmax(answer_end_scores)\n    \n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs.input_ids[0][start:end + 1]))\n    return answer.strip()\n\n# ------------------------\n# Retriever + QA Logic\n# ------------------------\n\nclass Retriever:\n    def __init__(self, index_dir: str):\n        \"\"\" Load the Faiss index and metadata \"\"\"\n        self.index_dir = Path(index_dir)\n        idx_path = self.index_dir / \"index.faiss\"\n        meta_path = self.index_dir / \"metadata.json\"\n        \n        if not idx_path.exists() or not meta_path.exists():\n            raise FileNotFoundError(f\"Index or metadata not found in {self.index_dir}\")\n        \n        self.index = faiss.read_index(str(idx_path))\n        with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n            js = json.load(f)\n        self.docs = js[\"docs\"]\n        self.metadatas = js[\"metadatas\"]\n        self.engine = EmbeddingEngine()\n\n    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n        \"\"\" Search for the most relevant text chunks using Faiss \"\"\"\n        qv = np.array(self.engine.embed_texts([query])).astype(\"float32\")\n        D, I = self.index.search(qv, k)\n        \n        results = []\n        for dist, idx in zip(D[0], I[0]):\n            if idx < 0 or idx >= len(self.docs):\n                continue\n            results.append({\"score\": float(dist), \"text\": self.docs[idx], \"meta\": self.metadatas[idx]})\n        \n        return results\n\ndef generate_answer(query: str, retrieved: List[Dict[str, Any]], conversation_history=None, language=\"en\") -> str:\n    \"\"\"\n    Generate an answer using either BERT QA model or retrieved text snippets.\n    \"\"\"\n    if conversation_history is None:\n        conversation_history = []\n    \n    # Add the current query to the conversation history\n    conversation_history.append({\"role\": \"user\", \"content\": query})\n    \n    # Combine context and conversation history\n    context = \"\\n\\n\".join([f\"(page: {r['meta']['page']}) {r['text'][:800]}\" for r in retrieved])\n    conversation_context = \"\\n\\n\".join([entry[\"content\"] for entry in conversation_history])\n    \n    # Use BERT for question answering\n    answer = qa_bert_model(query, context)\n    \n    # Fallback to concatenating retrieved texts if no answer is found\n    if not answer:\n        answer = \"Sorry, I couldn't find a precise answer. Here's some related information: \"\n        answer += \"\\n\\n\".join([r[\"text\"][:500] for r in retrieved[:3]])\n    \n    return answer\n\n# ------------------------\n# Main Logic\n# ------------------------\n\ndef main():\n    # Read PDF and create Faiss index\n    pdf_path = \"path_to_pdf\"\n    pages = read_pdf_by_page(pdf_path)\n    build_faiss_index(pages, \"index_dir\")\n\n    # Initialize retriever\n    retriever = Retriever(\"index_dir\")\n    \n    # Example query\n    query = \"What is the budget for energy projects?\"\n    \n    # Retrieve relevant chunks from the index\n    retrieved = retriever.search(query, k=3)\n    \n    # Generate answer using the retrieved chunks\n    answer = generate_answer(query, retrieved)\n    print(answer)\n    \nif __name__ == \"__main__\":\n    main()\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}